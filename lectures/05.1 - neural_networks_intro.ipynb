{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d379ee38-52ca-43bf-9af0-1be94d99d929",
   "metadata": {},
   "source": [
    "# Sieci neuronowe\n",
    "\n",
    "## Czym one właściwie są?\n",
    "Sieć neuronowa to taka funkcja, która uczy się zmieniać **rozkład jednej dystrybucji w rozkład drugiej**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a79bf22-c35d-4ad4-8abf-538831981d32",
   "metadata": {},
   "source": [
    "---\n",
    "## Czemu są fajne?\n",
    "\n",
    "* **Uniwersalny aproksymator** - nawet z jedną warstwą ukrytą potrafi w teorii przybliżyć dowolną funkcję ciągłą.\n",
    "* **Automatyczne uczenie cech** - sieć sama odkrywa hierarchię reprezentacji (piksele -> krawędzie -> kształty -> obiekty).\n",
    "* **Transfer learning** - modele wstępnie wytrenowane można szybko dostosować do nowych zadań.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9d916b-df5f-43b6-9f75-767aba914f99",
   "metadata": {},
   "source": [
    "---\n",
    "## Gdzie wady?\n",
    "* **Głodne danych i energii** - duże modele wymagają milionów przykładów i znacznych zasobów energetycznych.\n",
    "* **Wrażliwość на adversarial** - niewielkie perturbacje wejścia mogą drastycznie zmienić wynik.\n",
    "* **Reprodukcja bias** - jeśli dane są stronnicze, sieć skaluje niesprawiedliwość (np. rozpoznawanie twarzy).\n",
    "* **Ryzyko przeuczenia i wysoki próg sprzętowy** - nie zawsze opłacalne przy małych zbiorach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c495acde-e472-4042-87aa-da8a2c1992fe",
   "metadata": {},
   "source": [
    "---\n",
    "# Trochę historii\n",
    "## Szalone lata 50-te\n",
    "Są lata 50-te (1958) i **Frank Rosenblatt** wydaje paper o tytule *The Perceptron: A Probabilistic Model*\n",
    "O tym jak reprezentować dane tak jak są one w mózgu (w postaci połączeń, a nie danych typu zdjęcia).\n",
    "\n",
    "## Początki sieci komputerowych, ale koniec neuronowych?\n",
    "1969 - Powstaje pierwsza sieć komputerowa - ARPANET, a **Marvin Minsky i Seymour Papert** wydają książkę *Perceptrons: An Introduction to Computational Geometry*, która w głównej mierze krytykowała model perceptronu jako słaby (nie potrafi nawet odwzorować funkcji XOR). Po wydaniu tej książki hype na perceptrony się zatrzymał...\n",
    "\n",
    "## Backpropagation\n",
    "1974 - **Paul Werbos** napisał artykuł o tym jak zrobić algorytm *backpropagation* - kluczowy algorytm dzięki któremu mogliśmy aktualizować wagi szybciej i lepiej."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03f8212-4195-4130-9fa6-682d114c7a54",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n",
    "Dobra ale na czym to polega?\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*dsVvCeoxlU4GZ1y701Mo8g.png)\n",
    "\n",
    "Ale jak go napisać w kodzie? Nic prostszego!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85152156-0ba1-42c2-a2a4-548574311e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01798620996209156"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def activation_function(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, w1=0.0, w2=0.0, b=0.0, activation_function=lambda x: x):\n",
    "        self.w = [w1, w2]  # wagi\n",
    "        self.b = b         # bias\n",
    "        self.act_fun = activation_function\n",
    "\n",
    "    def predict(self, x):\n",
    "        z = self.w[0] * x[0] + self.w[1] * x[1] + self.b\n",
    "        return self.act_fun(z)\n",
    "\n",
    "perceptron = Perceptron(2, 1, 2, activation_function)\n",
    "perceptron.predict(x=[-4,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61595222-6fd6-4f61-ba9a-4acf0716b1e0",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP)\n",
    "\n",
    "Co to takiego? Jest to warstwa złożona z wielu małych perceptronów ułożonych w postacji warstw (ang. layers) w pełni połączonych (każdy element wejścia jest brany pod uwagę do każdej wagi sieci)\n",
    "\n",
    "![](https://blog.kakaocdn.net/dn/cDjlb7/btrZNJLs9Y1/LvUe42WG9Ry8NYOmhpMI2k/img.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
